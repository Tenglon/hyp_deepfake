{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2d5aac-0fad-4316-bef8-ac28245b4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class FFPPC23_DataLoader(Dataset):\n",
    "    \"\"\"facefrensics++ Feature dataset loader.\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            subset,\n",
    "            data_dir,\n",
    "          \n",
    "    ):\n",
    "        self.data_dir =data_dir\n",
    "\n",
    "        self.subset = subset\n",
    "        assert self.subset in [\"train\", \"val\", \"test\"]\n",
    "        video_id_path_dict = {}\n",
    "        video_id_path_dict[\"train\"] = os.path.join(self.data_dir,'train')\n",
    "        video_id_path_dict[\"val\"] = os.path.join(self.data_dir, \"val\")\n",
    "        video_id_path_dict[\"test\"] = os.path.join(self.data_dir, \"test\")\n",
    "\n",
    "        self.sample_len = 0\n",
    "\n",
    "        video_id_feat_dict[\"train\"] = self._get_files_with_extension(video_id_path_dict[\"train\"])\n",
    "        \n",
    "        self.sample_len = len(video_id_path_dict[self.subset])\n",
    "        \n",
    "\n",
    "    def _get_files_with_extension(self,root_folder, target_extension= 'frame.pt'):\n",
    "        result_files = []\n",
    "        for folder_path, _, files in os.walk(root_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(target_extension):\n",
    "                    result_files.append(os.path.join(folder_path, file))\n",
    "        return result_files\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.sample_len\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_feat_p = self.video_id_path_dict[self.subset][idx]\n",
    "\n",
    "        target = int(image_feat_p.split('/')[-1].split('_')[0])\n",
    "        frame_feats= torch.load(image_feat_p)\n",
    "        return feats, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37345201-22a4-4fcb-8420-d867e8522c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# import sys, os\n",
    "# path2add = '/home/achen/github/poincare-resnet-deepfake'\n",
    "# if (not (path2add in sys.path)) :\n",
    "#     sys.path.append(path2add)\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir= '/var/scratch/achen/VisualSearch/ffppc23/real/val'\n",
    "\n",
    "# from dataloader_ffppc23_retrieval import FFPPC23_DataLoader\n",
    "\n",
    "class Ffplusplusc23DataLoaderFactory:\n",
    "    train_set = FFPPC23_DataLoader(\n",
    "    subset = 'train',\n",
    "    data_dir=data_dir,\n",
    "    \n",
    "    )\n",
    "    \n",
    "\n",
    "    test_set = FFPPC23_DataLoader(\n",
    "    subset = 'val',\n",
    "    data_dir=data_dir,\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def create_train_loaders(cls, batch_size: int):\n",
    "        train_loader = DataLoader(\n",
    "            dataset=cls.train_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            dataset=cls.test_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08c0c41-d4ae-4e85-b7a0-edb128302141",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_factory \u001b[38;5;241m=\u001b[39m  Ffplusplusc23DataLoaderFactory\n\u001b[0;32m----> 2\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_train_loaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36mFfplusplusc23DataLoaderFactory.create_train_loaders\u001b[0;34m(cls, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_train_loaders\u001b[39m(\u001b[38;5;28mcls\u001b[39m, batch_size: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     36\u001b[0m         dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mtest_set,\n\u001b[1;32m     37\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     38\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loader, test_loader\n",
      "File \u001b[0;32m/var/scratch/achen/anaconda3/envs/clip/lib/python3.10/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/var/scratch/achen/anaconda3/envs/clip/lib/python3.10/site-packages/torch/utils/data/sampler.py:107\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "dataset_factory =  Ffplusplusc23DataLoaderFactory\n",
    "train_loader, test_loader = dataset_factory.create_train_loaders(\n",
    "        batch_size=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959a973-491d-49cf-b5f6-d2f1d48c56b6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e738f3-32ff-4741-8030-f7065eeb8d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "2841\n",
      "1437\n",
      "45436\n",
      "280\n",
      "2950\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "def list_directories(path):\n",
    "    # 确保路径存在\n",
    "    if not os.path.exists(path):\n",
    "        return \"路径不存在\"\n",
    "\n",
    "    directories = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    return directories\n",
    "\n",
    "def get_files_with_extension(root_folder, target_extension= 'frame.pt'):\n",
    "        result_files = []\n",
    "        for folder_path, _, files in os.walk(root_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(target_extension):\n",
    "                    result_files.append(os.path.join(folder_path, file))\n",
    "        return result_files\n",
    "\n",
    "p='/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings'\n",
    "# test train val \n",
    "# val:280个数据 \n",
    "# deepfakes \n",
    "# face2face  \n",
    "# faceswap\n",
    "# neuraltextures  \n",
    "\n",
    "# 替换成你想查看的路径\n",
    "for t in [\"test\", \"train\", \"val\" ]:\n",
    "    path = os.path.join(p,t)\n",
    "    # print(list_directories(path))\n",
    "    print(len(list_directories(path)))\n",
    "    print(len(get_files_with_extension(path)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf3303fe-deac-4dca-9d97-51dc2caa01fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n",
      "35\n",
      "372\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "s='deepfakes'\n",
    "tt='/var/scratch/achen/VisualSearch/ffppc23/{}/train'.format()\n",
    "print(len(get_files_with_extension(tt)))\n",
    "print(len(list_directories(tt)))\n",
    "tt='/var/scratch/achen/VisualSearch/ffppc23/{}/test'.format()\n",
    "print(len(get_files_with_extension(tt)))\n",
    "print(len(list_directories(tt)))\n",
    "tt='/var/scratch/achen/VisualSearch/ffppc23/{}/val'.format()\n",
    "print(len(get_files_with_extension(tt)))\n",
    "print(len(list_directories(tt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "861f562e-8e6d-4229-a11c-aa533fca2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/train\n",
      "5723\n",
      "180\n",
      "/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/val\n",
      "350\n",
      "35\n",
      "/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/test\n",
      "350\n",
      "35\n",
      "/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/train\n",
      "5638\n",
      "180\n",
      "/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/val\n",
      "350\n",
      "35\n",
      "/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/test\n",
      "349\n",
      "35\n",
      "/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/train\n",
      "5620\n",
      "179\n",
      "/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/val\n",
      "349\n",
      "35\n",
      "/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/test\n",
      "340\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_directories_with_string(src_path, dest_path, string):\n",
    "    # 确保源路径和目标路径存在\n",
    "    if not os.path.exists(src_path):\n",
    "        return \"源路径不存在\"\n",
    "    if not os.path.exists(dest_path):\n",
    "        os.makedirs(dest_path)\n",
    "\n",
    "    # 遍历源路径下的所有文件夹\n",
    "    for folder in os.listdir(src_path):\n",
    "        folder_path = os.path.join(src_path, folder)\n",
    "\n",
    "        # 检查是否为文件夹且名称中包含指定字符串\n",
    "        if os.path.isdir(folder_path) and string in folder:\n",
    "            # 构建目标文件夹路径\n",
    "            dest_folder_path = os.path.join(dest_path, folder)\n",
    "\n",
    "            # 复制文件夹\n",
    "            shutil.copytree(folder_path, dest_folder_path)\n",
    "\n",
    "    return \"复制完成\"\n",
    "\n",
    "# 示例用法\n",
    "for string in ['face2face','faceswap','neuraltextures' ]:\n",
    "    # string = \"deepfakes\"\n",
    "    print(\"==\"*10)\n",
    "    print(string)\n",
    "    for sets in ['train','val','test']:\n",
    "        src_path = \"/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/{}\".format(sets)\n",
    "        dest_path = \"/var/scratch/achen/VisualSearch/ffppc23/{}/{}\".format(string,sets)\n",
    "        print(src_path)\n",
    "        \n",
    "        \n",
    "        copy_directories_with_string(src_path, dest_path, string)\n",
    "    \n",
    "        tt='/var/scratch/achen/VisualSearch/ffppc23/{}/{}'.format(string,sets)\n",
    "        print(string,sets)\n",
    "        print('frame num')\n",
    "        print(len(get_files_with_extension(tt)))\n",
    "         print('video num')\n",
    "        print(len(list_directories(tt)))\n",
    "        print(\"==\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d127add4-b141-4d92-b9ac-2ce349b0acda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame num\n",
      "1529\n",
      "video num\n",
      "140\n",
      "====================\n",
      "frame num\n",
      "1452\n",
      "video num\n",
      "140\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_numeric_directories(src_path, dest_path):\n",
    "    # 确保源路径存在\n",
    "    if not os.path.exists(src_path):\n",
    "        return \"源路径不存在\"\n",
    "\n",
    "    # 如果目标路径不存在，则创建\n",
    "    if not os.path.exists(dest_path):\n",
    "        os.makedirs(dest_path)\n",
    "\n",
    "    # 遍历源路径下的所有项\n",
    "    for folder in os.listdir(src_path):\n",
    "        folder_path = os.path.join(src_path, folder)\n",
    "\n",
    "        # 检查是否为文件夹且名称仅由数字组成\n",
    "        if os.path.isdir(folder_path) and folder.isdigit():\n",
    "            dest_folder_path = os.path.join(dest_path, folder)\n",
    "\n",
    "            # 复制文件夹\n",
    "            shutil.copytree(folder_path, dest_folder_path)\n",
    "\n",
    "    return \"复制完成\"\n",
    "\n",
    "# 示例用法\n",
    "for sets in ['val','test']: \n",
    "    src_path = \"/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/{}\".format(sets)  # 源路径\n",
    "    dest_path = \"/var/scratch/achen/VisualSearch/ffppc23/real/{}\".format(sets)  # 目标路径\n",
    "    # copy_numeric_directories(src_path, dest_path)\n",
    "    # print(result)\n",
    "    print('frame num')\n",
    "    print(len(get_files_with_extension(dest_path)))\n",
    "    print('video num')\n",
    "    print(len(list_directories(dest_path)))\n",
    "    print(\"==\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20256247-075d-4337-a67e-0bd21cd32020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame num\n",
      "22753\n",
      "video num\n",
      "718\n",
      "====================\n",
      "frame num\n",
      "1529\n",
      "video num\n",
      "140\n",
      "====================\n",
      "frame num\n",
      "1452\n",
      "video num\n",
      "140\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "count =[]\n",
    "for sets in ['train','val','test']: \n",
    "    # src_path = \"/var/scratch/achen/VisualSearch/ffppc23_clip_embeddings/{}\".format(sets)  # 源路径\n",
    "    dest_path = \"/var/scratch/achen/VisualSearch/ffppc23/real/{}\".format(sets)  # 目标路径\n",
    "    # copy_numeric_directories(src_path, dest_path)\n",
    "    # print(result)\n",
    "    print('frame num')\n",
    "    print(len(get_files_with_extension(dest_path)))\n",
    "    print('video num')\n",
    "    print(len(list_directories(dest_path)))\n",
    "    count.extend(list_directories(dest_path))\n",
    "    print(\"==\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d772b70-c29d-4b43-af8f-8303f6761ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n",
      "998\n"
     ]
    }
   ],
   "source": [
    "print(len(count))\n",
    "print(len(set(count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64e1f176-5f6a-491d-9727-c4769d2a59bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除完成\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def delete_files_with_unwanted_extensions(folder_path, extensions):\n",
    "    # 确保文件夹路径存在\n",
    "    if not os.path.exists(folder_path):\n",
    "        return \"指定的文件夹路径不存在\"\n",
    "\n",
    "    # 遍历文件夹\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # 检查文件是否有不需要的后缀名\n",
    "            if not any(file.endswith(ext) for ext in extensions):\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)  # 删除文件\n",
    "\n",
    "    return \"删除完成\"\n",
    "\n",
    "# 示例用法\n",
    "folder_path =\"/var/scratch/achen/VisualSearch/ffppc23/real\"# 文件夹路径\n",
    "# extensions = [\".txt\", \".jpg\"]  # 保留的文件后缀列表\n",
    "extensions = ['_frame.pt' ]\n",
    "\n",
    "result = delete_files_with_unwanted_extensions(folder_path, extensions)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "320905d0-a5d4-4870-a7d8-6686ffb8db83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'数据集划分完成'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset(dataset_path, train_ratio=0.8, val_ratio=0.1):\n",
    "    # 确保数据集路径存在\n",
    "    if not os.path.exists(dataset_path):\n",
    "        return \"数据集路径不存在\"\n",
    "\n",
    "    # 创建train, val, test文件夹\n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "    # 遍历所有类别\n",
    "    for category in os.listdir(dataset_path):\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "\n",
    "        if os.path.isdir(category_path):\n",
    "            # 获取所有文件\n",
    "            files = os.listdir(category_path)\n",
    "            np.random.shuffle(files)  # 随机打乱文件顺序\n",
    "\n",
    "            # 计算划分点\n",
    "            train_split = int(len(files) * train_ratio)\n",
    "            val_split = train_split + int(len(files) * val_ratio)\n",
    "\n",
    "            # 划分数据集\n",
    "            for i, file in enumerate(files):\n",
    "                if i < train_split:\n",
    "                    subset = 'train'\n",
    "                elif i < val_split:\n",
    "                    subset = 'val'\n",
    "                else:\n",
    "                    subset = 'test'\n",
    "\n",
    "                # 创建子集中类别的文件夹\n",
    "                subset_path = os.path.join('/var/scratch/achen/VisualSearch/ffppc23/real/real_new_split', subset, category)\n",
    "                if not os.path.exists(subset_path):\n",
    "                    os.makedirs(subset_path)\n",
    "\n",
    "                # 复制文件到相应的子集文件夹\n",
    "                src_file = os.path.join(category_path, file)\n",
    "                dest_file = os.path.join(subset_path, file)\n",
    "                shutil.copy2(src_file, dest_file)\n",
    "                \n",
    "\n",
    "    return \"数据集划分完成\"\n",
    "\n",
    "# 示例用法\n",
    "dataset_path = \"/var/scratch/achen/VisualSearch/ffppc23/real/all_videos_998\"  # 你的数据集路径\n",
    "split_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4b6a8af-7a78-4b58-8dad-213a044f6a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "['176', '688', '706', '786', '947', '800', '533']\n"
     ]
    }
   ],
   "source": [
    "s1 =os.listdir('/var/scratch/achen/VisualSearch/ffppc23/real/real_new_split/val')\n",
    "s2 =os.listdir('/var/scratch/achen/VisualSearch/ffppc23/real/real_new_split/train')\n",
    "\n",
    "no = list(set(s2).difference(s1))\n",
    "print(len(no))\n",
    "print(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0cf852-c7df-487a-afa1-b1892b93bff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
